{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's my attempt to solve that problem. Below are the imports, as well as some dictionaries that will be used later. Might help to look at some definitions in my proposal if yu're not clear on anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "recession1_years = [ '2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007']\n",
    "recession2_years = [ '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019']\n",
    "schema_dict = { 'area_fips':str,  'own_code':str,  'industry_code':str,  'agglvl_code':str,  'size_code':str,  'year':int,  'qtr':int,  'disclosure_code':str,  'area_title':str,  'own_title':str,  'industry_title':str,  'agglvl_title':str,  'size_title':str,  'qtrly_estabs':int,  'month1_emplvl':int,  'month2_emplvl':int,  'month3_emplvl':int,  'total_qtrly_wages':int,  'taxable_qtrly_wages':int,  'qtrly_contributions':int,  'avg_wkly_wage':int,  'lq_disclosure_code':str,  'lq_qtrly_estabs':float,  'lq_month1_emplvl':float,  'lq_month1_emplv2':float,  'lq_month1_emplv3':float,  'lq_total_qtrly_wages':float,  'lq_taxable_qtrly_wages':float,  'lq_qrtly_contributions':float,  'oty_disclosure_code':str,  'oty_qtrly_estabs':int,  'oty_qtrly_estabs_pct_chg':float,  'oty_month1_emplvl_chg':int,  'oty_month1_emplvl_pct_chg':float,  'oty_month2_emplv_chg':int,  'oty_month2_emplvl_pct_chg':float,  'oty_month3_emplvl_chg':int,  'oty_month3_emplvl_pct_chg':float,  'oty_total_qtrly_wages_chg':int,  'oty_total_qtrly_wages_pct_chg':float,  'oty_taxable_qtrly_wages_chg':int,  'oty_taxable_qtrly_wages_pct_chg':float,  'oty_qrtly_contributions_chg':int,  'oty_qrtly_contributions_pct_chg':float,  'oty_avg_wkly_wage_chg':int,  'oty_avg_wkly_wage_pct_chg':float}  \n",
    "drop_columns = ['industry_code',  'own_code',  'size_code',  'disclosure_code',  'industry_title',  'own_title',  'size_title',  'lq_disclosure_code',  'oty_disclosure_code',  'oty_month1_emplvl_chg',  'oty_month2_emplvl_chg',  'oty_month3_emplvl_chg',  'oty_total_qtrly_wages_chg',  'oty_taxable_qtrly_wages_chg',  'oty_qtrly_contributions_chg',  'oty_avg_wkly_wage_chg',  'lq_qtrly_estabs_count',  'lq_month1_emplvl',  'lq_month2_emplvl',  'lq_month3_emplvl',  'lq_total_qtrly_wages',  'lq_taxable_qtrly_wages',  'lq_qtrly_contributions',  'oty_qtrly_estabs_count_chg',  'oty_qtrly_estabs_count_pct_chg',  'oty_month1_emplvl_pct',  'oty_month2_emplvl_pct',  'oty_month3_emplvl_pct',  'oty_total_qtrly_wages_pct',  'oty_taxable_qtrly_wages_chg',  'oty_qtrly_contributions_pct',  'oty_avg_wkly_wage_pct',  'oty_taxable_qtrly_wages_chg.1',  'lq_avg_wkly_wage',  'taxable_qtrly_wages',  'qtrly_contributions']\n",
    "import os\n",
    "os.chdir('/home/cj/Documents/dsi/capstones/report-card-recession')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to refer to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_qtrid(df):\n",
    "    '''\n",
    "    adds a column for the year and quarter.\n",
    "\n",
    "    params: df(dataframe)\n",
    "    returns: dataframe with column added'''\n",
    "    df['qtrid'] = df['year'] + (df['qtr']/4)\n",
    "    return df\n",
    "\n",
    "def import_one(year):\n",
    "    '''brings a single year's woth of data into a dataframe. Used for initial EDA. \n",
    "    Referenced in import_all\n",
    "\n",
    "    params: year(str)\n",
    "    returns: df(dataframe)'''\n",
    "    filepath = '../data/' + str(year) + '.csv'\n",
    "    #all relevant csvs are renamed with only the year\n",
    "    df = pd.read_csv(filepath, dtype = schema_dict)\n",
    "    #schema_dict is found in dictionaries.py\n",
    "    for column in drop_columns:\n",
    "        if column in df.columns:\n",
    "            df = df.drop([column], axis = 1)\n",
    "    return df\n",
    "\n",
    "def import_all(years):\n",
    "    '''combines as many years ofdata into a single dataframe, as well as adding quater id\n",
    "    References import_one and add_qtrid\n",
    "\n",
    "    params: years (list of str)\n",
    "    returns: df (dataframe)'''\n",
    "    df = import_one(years[0])\n",
    "    for year in years[1:]:\n",
    "        df = df.append(import_one(year))\n",
    "    df = add_qtrid(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the initial dataframe with all eight years of data appended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = import_all(recession2_years)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do a pivotable. Will give you two columns (str) for the area in question, and one for *each* quarter of data. Total job numbers only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_empl(df):\n",
    "    return df.pivot_table(columns = 'qtrid', values = 'month3_emplvl', index = ['area_fips', 'area_title'], aggfunc = np.sum)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Here's the part that works. the timeline dataframe is made, the nadir is calculated as the minimum of the entire timeline, and the quarter in whic it happens is saved as nadir_qtr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_nadir(s):\n",
    "    assert isinstance(s, pd.Series)\n",
    "    return s.min()\n",
    "\n",
    "def calc_nadir_qtr(s):\n",
    "    return s.argmin()\n",
    "\n",
    "def calc_pre_peak(s):\n",
    "    return s[ : s.argmin()].max()\n",
    "\n",
    "def calc_pre_peak_quarter(s):\n",
    "    try:\n",
    "        qtr = s[ : s.argmin()].argmax()\n",
    "    except:\n",
    "        qtr = None\n",
    "    return qtr\n",
    "\n",
    "def calc_post_peak(s):\n",
    "    return s[s.argmin() : ].max()\n",
    "\n",
    "def calc_post_peak_qtr(s):\n",
    "    return s[s.argmin() : ].argmax() + s.argmin()\n",
    "\n",
    "def calc_nadir(s):\n",
    "    assert isinstance(s, pd.Series)\n",
    "    return s.min()\n",
    "\n",
    "def calc_nadir_qtr(s):\n",
    "    return s.argmin()\n",
    "\n",
    "def calc_pre_peak(s):\n",
    "    return s[ : s.argmin()].max()\n",
    "\n",
    "def calc_pre_peak_quarter(s):\n",
    "    try:\n",
    "        qtr = s[ : s.argmin()].argmax()\n",
    "    except:\n",
    "        qtr = None\n",
    "    return qtr\n",
    "\n",
    "def calc_post_peak(s):\n",
    "    return s[s.argmin() : ].max()\n",
    "\n",
    "def calc_post_peak_qtr(s):\n",
    "    return s[s.argmin() : ].argmax() + s.argmin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = import_all(recession1_years)\n",
    "df = df.pivot_table(columns = 'qtrid', values = 'month3_emplvl', index = ['area_fips', 'area_title'], aggfunc = np.sum)\n",
    "df = df.reset_index()\n",
    "df2 = df.drop(columns = ['area_fips', 'area_title'])\n",
    "df2 = df2.reset_index()\n",
    "df2 = df2.drop(columns = 'index')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 4403 entries, 0 to 4402\nData columns (total 32 columns):\n #   Column   Non-Null Count  Dtype  \n---  ------   --------------  -----  \n 0   2000.25  4403 non-null   float64\n 1   2000.5   4403 non-null   float64\n 2   2000.75  4403 non-null   float64\n 3   2001.0   4403 non-null   float64\n 4   2001.25  4403 non-null   float64\n 5   2001.5   4403 non-null   float64\n 6   2001.75  4403 non-null   float64\n 7   2002.0   4403 non-null   float64\n 8   2002.25  4403 non-null   float64\n 9   2002.5   4403 non-null   float64\n 10  2002.75  4403 non-null   float64\n 11  2003.0   4403 non-null   float64\n 12  2003.25  4403 non-null   float64\n 13  2003.5   4403 non-null   float64\n 14  2003.75  4403 non-null   float64\n 15  2004.0   4403 non-null   float64\n 16  2004.25  4403 non-null   float64\n 17  2004.5   4403 non-null   float64\n 18  2004.75  4403 non-null   float64\n 19  2005.0   4403 non-null   float64\n 20  2005.25  4403 non-null   float64\n 21  2005.5   4403 non-null   float64\n 22  2005.75  4403 non-null   float64\n 23  2006.0   4403 non-null   float64\n 24  2006.25  4403 non-null   float64\n 25  2006.5   4403 non-null   float64\n 26  2006.75  4403 non-null   float64\n 27  2007.0   4403 non-null   float64\n 28  2007.25  4403 non-null   float64\n 29  2007.5   4403 non-null   float64\n 30  2007.75  4403 non-null   float64\n 31  2008.0   4403 non-null   float64\ndtypes: float64(32)\nmemory usage: 1.1 MB\n"
    }
   ],
   "source": [
    "#this specifies when the jobs numbers \"bottom-out\" during the recession\n",
    "# df = df.drop(columns = ['area_fips', 'area_title'])\n",
    "df2= df2.fillna(0)\n",
    "df2.info()\n",
    "\n",
    "nadir = df2.apply(lambda x: calc_nadir(x), axis=1).rename('nadir')\n",
    "#counts the number of quarters to the nadir since the beginning of the timeframe\n",
    "nadir_qtr = df2.apply(lambda x: calc_nadir_qtr(x), axis=1).rename('nadir_qtr')\n",
    "    \n",
    "#computes the highest points before and after the nadir, and captures the quarter count\n",
    "pre_peak = df2.apply(lambda x: calc_pre_peak(x), axis=1).rename('pre_peak')\n",
    "pre_peak_qtr = df2.apply(lambda x: calc_pre_peak_quarter(x), axis=1).rename('pre_peak_qtr')\n",
    "post_peak = df2.apply(lambda x: calc_post_peak(x), axis=1).rename('post_peak')\n",
    "post_peak_qtr = df2.apply(lambda x: calc_post_peak_qtr(x), axis=1).rename('post_peak_qtr')\n",
    "    \n",
    "#puts the computed points in a dataframe, joins with timeline\n",
    "df_results = pd.concat([df['area_fips'], nadir, nadir_qtr, pre_peak, pre_peak_qtr, post_peak, post_peak_qtr], axis=1)\n",
    "df = df.join(df_results, how = 'outer', rsuffix = '_derive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['recovery'] = (df['post_peak'] >= df['pre_peak']) *1\n",
    "df['full_decline'] = (df['nadir_qtr'] == 32) * -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "full_decline     0\nrecovery          \n0             2435\n1             1968",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>full_decline</th>\n      <th>0</th>\n    </tr>\n    <tr>\n      <th>recovery</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2435</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1968</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "pd.crosstab(df['recovery'], df['full_decline'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "    df_empl = df_empl.drop(columns=[2007.25, 2007.5, 2007.75, 2008.0,\n",
    "       2008.25, 2008.5, 2008.75, 2009.0, 2009.25, 2009.5, 2009.75, 2010.0, 2010.25, 2010.5, 2010.75, 2011.0, 2011.25, 2011.5, 2011.75,2012.0, 2012.25, 2012.5, 2012.75, 2013.0, 2013.25, 2013.5, 2013.75, 2014.0, 2014.25, 2014.5, 2014.75, 2015.0, 2015.25, 2015.5,\n",
    "       2015.75, 2016.0, 2016.25, 2016.5, 2016.75, 2017.0, 2017.25, 2017.5, 2017.75, 2018.0, 2018.25, 2018.5, 2018.75, 2019.0, 2019.25, 2019.5, 2019.75, 2020.0], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_empl['nadir_qtr'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a *very* messy attempt to brute force it. Do not feel obligated to preserve any of this. It doesn't work (at all) but it does give an idea of what I'm trying to accomplish here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiates the five columns I will be making from the nasty for loop\n",
    "df_empl['pre-peak'] = 0\n",
    "df_empl['pre-peak_qtr'] = 0\n",
    "df_empl['post-peak'] = 0\n",
    "df_empl['post-peak_qtr'] = 0\n",
    "df_empl['recov_qtr'] = 0\n",
    "\n",
    "#iterates through each row of the dataframe\n",
    "for index, row in df_empl.iterrows():\n",
    "    #save the nadir value in a variable\n",
    "    nadir = row['nadir']\n",
    "    #need an iterator and a var to store where the slice needs to happen on each row\n",
    "    counter = 0\n",
    "    slicer = 0\n",
    "    #make a np array from the row\n",
    "    row_array = np.array(row)\n",
    "    #iterate through each value in the array\n",
    "    for num in row_array:\n",
    "        #check each value to see if it is the same as the nadir. If so, set the slicer var.\n",
    "        if num == nadir:\n",
    "            slicer = counter\n",
    "            break\n",
    "        else:\n",
    "            counter += 1\n",
    "    \n",
    "    #break the array into two peices, before and after the nadir\n",
    "    pre_array = row_array[2:slicer]\n",
    "    post_array = row_array[slicer +1:]\n",
    "\n",
    "    #set local vars for the pre and post peaks, and set them in the dataframe\n",
    "    if pre_array.size:\n",
    "        pre_peak = np.max(pre_array)\n",
    "        row['pre_peak'] = np.max(pre_array)\n",
    "    else:\n",
    "        row['pre_peak'] = 0\n",
    "    post_peak = np.max(post_array)\n",
    "    row['post-peak'] = np.max(post_array)\n",
    "    \n",
    "    #create iterators and local vars for the pre- and -post peak quarters\n",
    "    precounter = 0\n",
    "    postcounter = 0\n",
    "    pre_qtr = 0\n",
    "    post_qtr = 0\n",
    "    \n",
    "    #iterate through the array to find the number of the column with the pre-peak\n",
    "    for num in row_array:\n",
    "        if num == pre_peak:\n",
    "            pre_qtr = precounter\n",
    "            break\n",
    "        else:\n",
    "            precounter += 1\n",
    "    #use the number to calculate the column where the pre-peak occurs\n",
    "    row['pre-peak_qtr'] = 2000.25 + (pre_qtr / 4)\n",
    "    \n",
    "    #do the same for the post-peak(in reverse)\n",
    "    for num in reversed(row_array):\n",
    "        if num == post_peak:\n",
    "            post_qtr = 36 - postcounter\n",
    "        else:\n",
    "            postcounter += 1\n",
    "    row['post-peak_qtr'] = 2000.25 + (post_qtr / 4)\n",
    "    \n",
    "    #calculate the recovery quarter- the point at which the area has surpassed its pre-nadir job numbers\n",
    "    recovcounter = 0\n",
    "    for num in post_array:\n",
    "        if num >= pre_peak:\n",
    "            recov_qtr = recovcounter\n",
    "            break\n",
    "        else:\n",
    "            recovcounter += 1\n",
    "    row['recov_qtr'] = 2000.25 + (recovcounter /4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_empl.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primary Target: did the area recover at all? Did they reach their former job numbers before the next recession start?\n",
    "df_empl['recovery'] = (df_empl['post-peak'] >= df_empl['pre-peak']) * 1\n",
    "\n",
    "#Secondary Targets: Time of decline (between peak and nadir) and time to recover (nadir to recovery)\n",
    "df_empl['decline_qtrs'] = (df_empl['nadir_qtr'] - df_empl['pre-peak_qtr']) * 4\n",
    "df_empl['recovery_qtrs'] = (df_empl['recov_qtr'] - df_empl['nadir_qtr']) * 4\n",
    "\n",
    "#Tertiary Target: job growth. Difference b/w pre-peak and post-peak\n",
    "df_empl['emp_delta'] = df_empl['post-peak'] - df_empl['pre-peak'] \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_empl.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_empl "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_empl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = import_all(recession2_years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = create_df_empl(df2)\n",
    "df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = import_all(recession2_years)\n",
    "df = df.drop(drop_columns, axis =1)\n",
    "df = create_df_empl(df)\n",
    "df = df.dropna(axis = 0)\n",
    "df = df.reset_index()\n",
    "df['nadir'] = df.iloc[:,9:].min(axis=1)\n",
    "df['nadir_qtr'] = df.iloc[:,9:].idxmin(axis=1)\n",
    "df['nadir_qtr_ct'] = (df['nadir_qtr'] - 2007.25) * 4\n",
    "df['pre-peak'] = 0\n",
    "df['post-peak'] = 0\n",
    "for index, row in df.iterrows():\n",
    "    slicer = int(row['nadir_qtr_ct'] + 3)\n",
    "    pre_peak = row.iloc[2:slicer].max()\n",
    "    df.iloc[index,53] = pre_peak\n",
    "    post_peak = row.iloc[slicer:].max()\n",
    "    df.iloc[index,54] = post_peak\n",
    "df['recovery'] = df['post-peak'] >= df['pre-peak']\n",
    "df['delta'] = df['post-peak'] - df['pre-peak']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python38364bitbaseconda55feb2f7aa2c4cc0b25b94f79384851f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}