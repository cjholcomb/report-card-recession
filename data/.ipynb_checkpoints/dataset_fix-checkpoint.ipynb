{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "recession1_years = [ '2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007']\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "drop_columns = ['industry_code',  'own_code',  'size_code',  'disclosure_code',  'industry_title',  'own_title',  'size_title',  'lq_disclosure_code',  'oty_disclosure_code',  'oty_month1_emplvl_chg',  'oty_month2_emplvl_chg',  'oty_month3_emplvl_chg',  'oty_total_qtrly_wages_chg',  'oty_taxable_qtrly_wages_chg',  'oty_qtrly_contributions_chg',  'oty_avg_wkly_wage_chg',  'lq_qtrly_estabs_count',  'lq_month1_emplvl',  'lq_month2_emplvl',  'lq_month3_emplvl',  'lq_total_qtrly_wages',  'lq_taxable_qtrly_wages',  'lq_qtrly_contributions',  'oty_qtrly_estabs_count_chg',  'oty_qtrly_estabs_count_pct_chg',  'oty_month1_emplvl_pct',  'oty_month2_emplvl_pct',  'oty_month3_emplvl_pct',  'oty_total_qtrly_wages_pct',  'oty_taxable_qtrly_wages_chg',  'oty_qtrly_contributions_pct',  'oty_avg_wkly_wage_pct',  'oty_taxable_qtrly_wages_chg.1',  'lq_avg_wkly_wage',  'taxable_qtrly_wages',  'qtrly_contributions']\n",
    "\n",
    "schema_dict = { 'area_fips':str,  'own_code':str,  'industry_code':str,  'agglvl_code':str,  'size_code':str,  'year':int,  'qtr':int,  'disclosure_code':str,  'area_title':str,  'own_title':str,  'industry_title':str,  'agglvl_title':str,  'size_title':str,  'qtrly_estabs':int,  'month1_emplvl':int,  'month2_emplvl':int,  'month3_emplvl':int,  'total_qtrly_wages':int,  'taxable_qtrly_wages':int,  'qtrly_contributions':int,  'avg_wkly_wage':int,  'lq_disclosure_code':str,  'lq_qtrly_estabs':float,  'lq_month1_emplvl':float,  'lq_month1_emplv2':float,  'lq_month1_emplv3':float,  'lq_total_qtrly_wages':float,  'lq_taxable_qtrly_wages':float,  'lq_qrtly_contributions':float,  'oty_disclosure_code':str,  'oty_qtrly_estabs':int,  'oty_qtrly_estabs_pct_chg':float,  'oty_month1_emplvl_chg':int,  'oty_month1_emplvl_pct_chg':float,  'oty_month2_emplv_chg':int,  'oty_month2_emplvl_pct_chg':float,  'oty_month3_emplvl_chg':int,  'oty_month3_emplvl_pct_chg':float,  'oty_total_qtrly_wages_chg':int,  'oty_total_qtrly_wages_pct_chg':float,  'oty_taxable_qtrly_wages_chg':int,  'oty_taxable_qtrly_wages_pct_chg':float,  'oty_qrtly_contributions_chg':int,  'oty_qrtly_contributions_pct_chg':float,  'oty_avg_wkly_wage_chg':int,  'oty_avg_wkly_wage_pct_chg':float}   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This file takes the quarterly raw data from the QCEW and turns them into quarterly timelines. From here, we can compute target variables.\n",
    "\n",
    "def add_qtrid(df):\n",
    "    '''\n",
    "    adds a column for the year and quarter.\n",
    "\n",
    "    params: df(dataframe)\n",
    "    returns: dataframe with column added'''\n",
    "    df['qtrid'] = df['year'] + (df['qtr']/4)\n",
    "    return df\n",
    "\n",
    "def import_one(year):\n",
    "    '''brings a single year's woth of data into a dataframe. Used for initial EDA. \n",
    "    Referenced in import_all\n",
    "\n",
    "    params: year(str)\n",
    "    returns: df(dataframe)'''\n",
    "    filepath =  str(year) + '.csv'\n",
    "    #all relevant csvs are renamed with only the year\n",
    "    df = pd.read_csv(filepath, dtype = schema_dict)\n",
    "    #schema_dict is found in dictionaries.py\n",
    "    for column in drop_columns:\n",
    "        if column in df.columns:\n",
    "            df = df.drop([column], axis = 1)\n",
    "    return df\n",
    "\n",
    "def import_all(years):\n",
    "    '''combines as many years ofdata into a single dataframe, as well as adding quater id\n",
    "    References import_one and add_qtrid\n",
    "\n",
    "    params: years (list of str)\n",
    "    returns: df (dataframe)'''\n",
    "    df = import_one(years[0])\n",
    "    for year in years[1:]:\n",
    "        df = df.append(import_one(year))\n",
    "    df = add_qtrid(df)\n",
    "    return df\n",
    "\n",
    "\n",
    "def third_quarter(index):\n",
    "    '''imports only every 3rd quarter row- required to import entire dataset (too large)\n",
    "    Referenced in feature_space\n",
    "\n",
    "    params: index, int\n",
    "    returns: boolean'''\n",
    "    if index == 0:\n",
    "        return False\n",
    "    elif (index - 3) % 4 == 0:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Help functions for target calculation ###\n",
    "\n",
    "def calc_nadir(s):\n",
    "    assert isinstance(s, pd.Series)\n",
    "    return s.min()\n",
    "\n",
    "def calc_nadir_qtr(s):\n",
    "    return s.argmin()\n",
    "\n",
    "def calc_pre_peak(s):\n",
    "    return s[ : s.argmin()].max()\n",
    "\n",
    "def calc_pre_peak_quarter(s):\n",
    "    try:\n",
    "        qtr = s[ : s.argmin()].argmax()\n",
    "    except:\n",
    "        qtr = None\n",
    "    return qtr\n",
    "\n",
    "def calc_post_peak(s):\n",
    "    return s[s.argmin() : ].max()\n",
    "\n",
    "def calc_post_peak_qtr(s):\n",
    "    return s[s.argmin() : ].argmax() + s.argmin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_timeline_2001(variable):\n",
    "    '''produces a timeline dataframe(and exports to json) for 2001\n",
    "\n",
    "    params: variable, str, one of ['month3_emplvel' (employment), 'avg_wkly_wage' (wages)]\n",
    "    returns: df, Dataframe'''\n",
    "    \n",
    "    df = import_all(recession1_years)\n",
    "    df = df.pivot_table(columns = 'qtrid', values = variable, index = ['area_fips', 'area_title'], aggfunc = np.sum)\n",
    "    df = df.reset_index()\n",
    "    #fill nans\n",
    "    df = df.fillna(0)\n",
    "    df2 = df.drop(columns = ['area_fips', 'area_title'])\n",
    "    df2 = df2.reset_index()\n",
    "    df2 = df2.drop(columns = 'index')\n",
    "    df2 = df2.fillna(0)\n",
    "\n",
    "    \n",
    "#     #this specifies when the jobs numbers \"bottom-out\" during the recession\n",
    "#     nadir = df2.iloc[:,6:].apply(lambda x: calc_nadir(x), axis=1).rename('nadir')\n",
    "    \n",
    "#     #counts the number of quarters to the nadir since the beginning of the timeframe\n",
    "#     nadir_qtr = df2.iloc[:,6:].apply(lambda x: calc_nadir_qtr(x), axis=1).rename('nadir_qtr')\n",
    "    \n",
    "#     #computes the highest points before and after the nadir, and captures the quarter count\n",
    "#     pre_peak = df2.apply(lambda x: calc_pre_peak(x), axis=1).rename('pre_peak')\n",
    "#     pre_peak_qtr = df2.apply(lambda x: calc_pre_peak_quarter(x), axis=1).rename('pre_peak_qtr')\n",
    "#     post_peak = df2.apply(lambda x: calc_post_peak(x), axis=1).rename('post_peak')\n",
    "#     post_peak_qtr = df2.apply(lambda x: calc_post_peak_qtr(x), axis=1).rename('post_peak_qtr')\n",
    "    \n",
    "\n",
    "    df['nadir'] = df.iloc[:,6:].min(axis=1)\n",
    "    df['nadir_qtr'] = df.iloc[:,6:].idxmin(axis=1).apply(lambda x: df.columns.get_loc(x))\n",
    "    df['new'] = [df.iloc[i].values for i in df.index]\n",
    "    df['pre_peak'] = df.apply(lambda x: max(x['new'][0:x['nadir_qtr']]), axis=1)\n",
    "    df['post_peak'] = df.apply(lambda x: max(x['new'][x['nadir_qtr']:]), axis=1)\n",
    "    df['pre_peak_qtr'] = pd.Series([s[i] for i, s in zip(df.index, df['pre_peak'].apply(\n",
    "        lambda x: [i for i in (df.iloc[:,0:-6] == x)\n",
    "                   .idxmax(axis=1)]))]).apply(lambda x: df.columns.get_loc(x))\n",
    "    df['post_peak_qtr'] = pd.Series([s[i] for i, s in zip(df.index, df['post_peak'].apply(\n",
    "        lambda x: [i for i in (df.iloc[:,0:-6] == x)\n",
    "                   .idxmax(axis=1)]))]).apply(lambda x: df.columns.get_loc(x))\n",
    "    df_new = df[['nadir', 'nadir_qtr', 'pre_peak', 'pre_peak_qtr', 'post_peak', 'post_peak_qtr']]\n",
    "    df_new\n",
    "    \n",
    "    #puts the computed points in a dataframe, joins with timeline\n",
    "#     df_results = pd.concat([df['area_fips'], nadir, nadir_qtr, pre_peak, pre_peak_qtr, post_peak, post_peak_qtr], axis=1)\n",
    "#     df = df.join(df2, how = 'outer', rsuffix = '_derive')\n",
    "    \n",
    "#     #PRIMARY TARGET: did the area decline the entire time(-1), did it start growing again but not avhieve it's former numbers(0), or did it grow and recover(1)?\n",
    "#     df['recovery'] = (df['post_peak'] >= df['pre_peak']) *1\n",
    "\n",
    "#     #SECONDARY TARGET: How long did the jobs numbers decline?\n",
    "#     df['decline'] = (df['nadir_qtr'] - df['pre_peak_qtr'])\n",
    "    \n",
    "#     #TERTIARY TARGET different in before/after jobs numbers\n",
    "#     df['delta'] = df['post_peak'] - df['pre_peak']\n",
    "    \n",
    "    #export the data\n",
    "#     df.to_json('data/Recession1_timeline.json')\n",
    "#     df = df.fillna(0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "reduction operation 'argmin' not allowed for this dtype",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-2fbc3728df3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcreate_timeline_2001\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'month3_emplvl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-a9a877ad8764>\u001b[0m in \u001b[0;36mcreate_timeline_2001\u001b[0;34m(variable)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'nadir'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'nadir_qtr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midxmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'new'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pre_peak'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'new'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'nadir_qtr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36midxmin\u001b[0;34m(self, axis, skipna)\u001b[0m\n\u001b[1;32m   8786\u001b[0m         \"\"\"\n\u001b[1;32m   8787\u001b[0m         \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 8788\u001b[0;31m         \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnanops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnanargmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   8789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8790\u001b[0m         \u001b[0;31m# indices will always be np.ndarray since axis is not None and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36m_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobj_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mf_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nan\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 raise TypeError(\n\u001b[0m\u001b[1;32m     67\u001b[0m                     \u001b[0;34mf\"reduction operation '{f_name}' not allowed for this dtype\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 )\n",
      "\u001b[0;31mTypeError\u001b[0m: reduction operation 'argmin' not allowed for this dtype"
     ]
    }
   ],
   "source": [
    "create_timeline_2001('month3_emplvl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python38364bitbaseconda55feb2f7aa2c4cc0b25b94f79384851f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
