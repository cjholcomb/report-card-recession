{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dictionaries import *\n",
    "from helper_functions import *\n",
    "from produce_datasets import *\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_qtrid(df):\n",
    "    '''\n",
    "    adds a column for the year and quarter.\n",
    "\n",
    "    params: df(dataframe)\n",
    "    returns: dataframe with column added'''\n",
    "    df['qtrid'] = df['year'] + (df['qtr']/4)\n",
    "    return df\n",
    "\n",
    "def import_one(year):\n",
    "    '''brings a single year's woth of data into a dataframe. Used for initial EDA. \n",
    "    Referenced in import_all\n",
    "\n",
    "    params: year(str)\n",
    "    returns: df(dataframe)'''\n",
    "    filepath = '../data/area_files/' + str(year) + '.csv'\n",
    "    #all relevant csvs are renamed with only the year\n",
    "    df = pd.read_csv(filepath, dtype = schema_dict)\n",
    "    #schema_dict is found in dictionaries.py\n",
    "    for column in drop_columns:\n",
    "        if column in df.columns:\n",
    "            df = df.drop([column], axis = 1)\n",
    "    return df\n",
    "\n",
    "def import_all(years):\n",
    "    '''combines as many years ofdata into a single dataframe, as well as adding quater id\n",
    "    References import_one and add_qtrid\n",
    "\n",
    "    params: years (list of str)\n",
    "    returns: df (dataframe)'''\n",
    "    df = import_one(years[0])\n",
    "    for year in years[1:]:\n",
    "        df = df.append(import_one(year))\n",
    "    df = add_qtrid(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_timeline_2001(variable):\n",
    "    '''produces a dataframe of the 2001 recession timeline.\n",
    "    \n",
    "    .0Used to compute targets\n",
    "\n",
    "    params: variable, str, one of ['month3_emplvel' (employment), 'avg_wkly_wage' (wages)]\n",
    "    returns: df, Dataframe\n",
    "    exports a json file (used in plotting results)\n",
    "    '''\n",
    "    \n",
    "    #create a dataframe of the years in question\n",
    "    df = import_all(recession1_years)\n",
    "\n",
    "    #drop 'unknown or undefined' areas\n",
    "    df = df[~df['area_fips'].str.contains(\"999\")]\n",
    "    \n",
    "    #pivots the table to arrange quarters in columns, drops extraneous variables.\n",
    "    df = df.pivot_table(columns = 'qtrid', values = variable, index = ['area_fips', 'area_title'], aggfunc = np.sum)\n",
    "    df = df.reset_index()\n",
    "    \n",
    "    #fill nans\n",
    "    df = df.fillna(0)\n",
    "\n",
    "    #creates a secondary dataframe with only timeline variables\n",
    "    df2 = df.drop(columns = ['area_fips', 'area_title'])\n",
    "    df2 = df2.reset_index()\n",
    "    \n",
    "    #drops the index so that all calculations are free of any type mismatches\n",
    "    df2 = df2.drop(columns = 'index')\n",
    "    df2 = df2.fillna(0)\n",
    "\n",
    "### DEPRECIATED CODE #### \n",
    "\n",
    "#     #this specifies when the jobs numbers \"bottom-out\" during the recession\n",
    "#     nadir = df2.iloc[:,6:].apply(lambda x: calc_nadir(x), axis=1).rename('nadir')\n",
    "    \n",
    "#     #counts the number of quarters to the nadir since the beginning of the timeframe\n",
    "#     nadir_qtr = df2.iloc[:,6:].apply(lambda x: calc_nadir_qtr(x), axis=1).rename('nadir_qtr')\n",
    "    \n",
    "#     #computes the highest points before and after the nadir, and captures the quarter count\n",
    "#     pre_peak = df2.apply(lambda x: calc_pre_peak(x), axis=1).rename('pre_peak')\n",
    "#     pre_peak_qtr = df2.apply(lambda x: calc_pre_peak_quarter(x), axis=1).rename('pre_peak_qtr')\n",
    "#     post_peak = df2.apply(lambda x: calc_post_peak(x), axis=1).rename('post_peak')\n",
    "#     post_peak_qtr = df2.apply(lambda x: calc_post_peak_qtr(x), axis=1).rename('post_peak_qtr')\n",
    "\n",
    "### DEPRECIATED CODE #### \n",
    "    \n",
    "    #specifies the lowest job numbers during the recession. Disregards quarters before the recession event. \n",
    "    df2['nadir'] = df2.iloc[:,6:].min(axis=1)\n",
    "\n",
    "    #specifies which quarter the nadir occured.\n",
    "    df2['nadir_qtr'] = df2.iloc[:,6:].idxmin(axis=1).apply(lambda x: df.columns.get_loc(x))\n",
    "    \n",
    "    #creates a column to store indices for lookup.\n",
    "    df2['new'] = [df2.iloc[i].values for i in df.index]\n",
    "    \n",
    "    #specifies the highest job numbers *before* the nadir.\n",
    "    df2['pre_peak'] = df2.apply(lambda x: max(x['new'][0:x['nadir_qtr']]), axis=1)\n",
    "    \n",
    "    #specifies the highest job numbers *after* the nadir\n",
    "    df2['post_peak'] = df2.apply(lambda x: max(x['new'][x['nadir_qtr']:]), axis=1)\n",
    "    \n",
    "    #specifies which quarter the pre-peak occurred.\n",
    "    df2['pre_peak_qtr'] = pd.Series([s[i] for i, s in zip(df2.index, df2['pre_peak'].apply(\n",
    "        lambda x: [i for i in (df2.iloc[:,0:-6] == x)\n",
    "                   .idxmax(axis=1)]))]).apply(lambda x: df2.columns.get_loc(x))\n",
    "    \n",
    "    #specifies which quarter the post-peak occurred.\n",
    "    df2['post_peak_qtr'] = pd.Series([s[i] for i, s in zip(df2.index, df2['post_peak'].apply(\n",
    "        lambda x: [i for i in (df2.iloc[:,0:-6] == x)\n",
    "                   .idxmax(axis=1)]))]).apply(lambda x: df2.columns.get_loc(x))\n",
    "    \n",
    "    #creates a new dataset to store derived fields\n",
    "    df_new = df2[['nadir', 'nadir_qtr', 'pre_peak', 'pre_peak_qtr', 'post_peak', 'post_peak_qtr', 'new']]\n",
    "    \n",
    "    #puts the computed points in a dataframe, joins with timeline\n",
    "    df = df.join(df_new, how = 'outer', rsuffix = '_derive')\n",
    "    \n",
    "    #PRIMARY TARGET: did the area decline the entire time(-1), did it start growing again but not avhieve it's former numbers(0), or did it grow and recover(1)?\n",
    "    df['recovery'] = (df['post_peak'] >= df['pre_peak']) *1\n",
    "\n",
    "    #SECONDARY TARGET: How long did the jobs numbers decline?\n",
    "    df['decline'] = (df['nadir_qtr'] - df['pre_peak_qtr'])\n",
    "    \n",
    "    #TERTIARY TARGET: different in before/after jobs numbers\n",
    "    df['delta'] = df['post_peak'] - df['pre_peak']\n",
    "    \n",
    "    #export the data\n",
    "    # df.to_json('data/Recession1_timeline.json')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = create_timeline_2001('month3_emplvl')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nInt64Index: 4351 entries, 0 to 4350\nData columns (total 43 columns):\n #   Column         Non-Null Count  Dtype  \n---  ------         --------------  -----  \n 0   area_fips      4351 non-null   object \n 1   area_title     4351 non-null   object \n 2   2000.25        4351 non-null   float64\n 3   2000.5         4351 non-null   float64\n 4   2000.75        4351 non-null   float64\n 5   2001.0         4351 non-null   float64\n 6   2001.25        4351 non-null   float64\n 7   2001.5         4351 non-null   float64\n 8   2001.75        4351 non-null   float64\n 9   2002.0         4351 non-null   float64\n 10  2002.25        4351 non-null   float64\n 11  2002.5         4351 non-null   float64\n 12  2002.75        4351 non-null   float64\n 13  2003.0         4351 non-null   float64\n 14  2003.25        4351 non-null   float64\n 15  2003.5         4351 non-null   float64\n 16  2003.75        4351 non-null   float64\n 17  2004.0         4351 non-null   float64\n 18  2004.25        4351 non-null   float64\n 19  2004.5         4351 non-null   float64\n 20  2004.75        4351 non-null   float64\n 21  2005.0         4351 non-null   float64\n 22  2005.25        4351 non-null   float64\n 23  2005.5         4351 non-null   float64\n 24  2005.75        4351 non-null   float64\n 25  2006.0         4351 non-null   float64\n 26  2006.25        4351 non-null   float64\n 27  2006.5         4351 non-null   float64\n 28  2006.75        4351 non-null   float64\n 29  2007.0         4351 non-null   float64\n 30  2007.25        4351 non-null   float64\n 31  2007.5         4351 non-null   float64\n 32  2007.75        4351 non-null   float64\n 33  2008.0         4351 non-null   float64\n 34  nadir          4351 non-null   float64\n 35  nadir_qtr      4351 non-null   int64  \n 36  pre_peak       4351 non-null   float64\n 37  pre_peak_qtr   4351 non-null   int64  \n 38  post_peak      4351 non-null   float64\n 39  post_peak_qtr  4351 non-null   int64  \n 40  recovery       4351 non-null   int64  \n 41  decline        4351 non-null   int64  \n 42  delta          4351 non-null   float64\ndtypes: float64(36), int64(5), object(2)\nmemory usage: 1.6+ MB\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_samp = df[df['recovery'] == 1].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_samp['recovery_num'] = df_samp.apply(lambda x: (x['new'][x['nadir_qtr']:] > x['pre_peak']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_samp['recovery_list_len'] = len(df_samp['new'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-9d3242829317>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_samp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'recovery_qtr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_samp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'recovery_num'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_samp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'recovery_num'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "\n",
    "df_samp['recovery_qtr'] = df_samp['recovery_num'].index(next(filter(lambda i: i !=0, df_samp['recovery_num'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# biggest_vals = df_empl.transpose().iloc[0:8,:].max(axis=0)\n",
    "# ending = (df_empl.transpose() > biggest_vals).idxmax()\n",
    "# biggest_idx = df_wages.T.iloc[0:8,:].idxmax()"
   ]
  }
 ]
}